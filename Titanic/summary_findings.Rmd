---
title: "Predicting Survival on the Titanic:"
subtitle: "A Comparision of Methods"
output:
  github_document:
    fig_width: 12
    fig_height: 8
---

```{r load packages and data, include = FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(randomForest)
library(gbm)
library(tree)
library(InformationValue)
options(dplyr.summarise.inform = FALSE)

train <- read.csv("/Users/hanson377/Documents/GitHub/kaggle_projects/Titanic/data/train.csv")
train <- na.omit(train)
train <- train %>% filter(Embarked != '')
```  

**Objective:**
Compare the predictive power of the following methods:  

1. Logistic Regression  
2. Classification Tree
3. Random Forest  
4. Gradient Boosting

After comparing the above methods, apply model to test data supplied by Kaggle and upload to see how I rank among other Kaggle Users.


## **Exploratory Data Analysis**
First, we will explore our data to identify key variables that we might want to include in any predictive model.  To do this most efficiently, we will first define a few functions to quickly cycle through and visualize differences across factor variables and their values as they relate to survival rates.


```{r create functions for summarizing and plotting summarization, echo = FALSE, warnings = FALSE}
calc_summary <- function(grouping_var) {
  grouping_var <- enquo(grouping_var)

  summary <- train %>% group_by(!!grouping_var) %>% summarise(survival_rate = mean(Survived), n = n_distinct(PassengerId)) %>% select(survival_rate,n,factor_value=!!grouping_var)

  return(summary)
}

plot_view <- function(summary_data,factor_title) {

  ggplot(summary_data, aes(x=factor_value,y=survival_rate,fill=factor(factor_value))) + geom_bar(stat='identity') + ggtitle(factor_title) + scale_y_continuous(labels=scales::percent) + xlab('') + ylab('Survival Rate') + geom_text(aes(label=n),position=position_dodge(0.9),vjust=-0.25) + theme(legend.position='none')
}
```  

Now that we have some functions, we can quickly calculate survival rates and visualize them alongside eachother.  

```{r compare survival rates across values for key factor variables, echo = FALSE}
pclass <- calc_summary(Pclass)
pclass_view <- plot_view(pclass,"P-Class")

sex <- calc_summary(Sex)
sex_view <- plot_view(sex, "Sex")

embarked <- calc_summary(Embarked)
embarked_view <- plot_view(embarked, "Location Embarked From")

sib <- calc_summary(SibSp)
sib_view <- plot_view(sib, 'Sibling')

parch <- calc_summary(Parch)
parch_view <- plot_view(parch, 'Parch Thing')

grid.arrange(pclass_view, sex_view, embarked_view,sib_view, parch_view, nrow = 2)
```

```{r adjust vars, include = FALSE, echo = FALSE}
prepare <- function(data) { ## write function that will do some variable engineering which we can easily apply to both the training and test data set
data$cabin_alt <- substr(data$Cabin,0,1)
data$cabin_alt <- ifelse(data$cabin_alt == '','no cabin',data$cabin_alt)

data$cabin_status <- ifelse(data$cabin_alt == 'no cabin','no cabin','cabin')

data$cabin_status <- ifelse(data$cabin_alt == 'no cabin','no cabin','cabin')

data$Parch_alt <- ifelse(data$Parch == 0,'0','Not Zero')
data$SibSp_alt <- ifelse(data$SibSp == 0,'0','Not Zero')

data$test <- str_split(data$Name,',')

data$title <- unlist(lapply(strsplit(as.character(data$Name), ","), '[[', 2))
data$title <- unlist(lapply(strsplit(as.character(data$title), "[.]"), '[[', 1))
data$title <- trimws(data$title)

data$title <- ifelse(data$title == 'Ms', 'Miss', data$title)
data$title <- ifelse(data$title == 'Mlle', 'Miss', data$title)

data$title <- ifelse(data$title == 'Mme', 'Mrs', data$title)
data$title <- ifelse(data$title == 'Lady', 'Mrs', data$title)
data$title <- ifelse(data$title == 'the Countess', 'Mrs', data$title)
data$title <- ifelse(data$title == 'Dona', 'Mrs', data$title)

data$title <- ifelse(data$title == 'Sir', 'Mr', data$title)
data$title <- ifelse(data$title == 'Capt', 'Mr', data$title)
data$title <- ifelse(data$title == 'Col', 'Mr', data$title)
data$title <- ifelse(data$title == 'Don', 'Mr', data$title)
data$title <- ifelse(data$title == 'Major', 'Mr', data$title)
data$title <- ifelse(data$title == 'Rev', 'Mr', data$title)
data$title <- ifelse(data$title == 'Jonkheer', 'Mr', data$title)
data$title <- ifelse(data$title == 'Dr', 'Mr', data$title)


data$ticket_alt <- unlist(lapply(strsplit(as.character(data$Ticket), " "), '[[', 1))
data$ticket_alt <- unlist(lapply(strsplit(as.character(data$ticket_alt), "/"), '[[', 1))
data$ticket_alt <- str_replace(data$ticket_alt, '[.]', '')

data$Pclass <- factor(data$Pclass)
data$Sex <- factor(data$Sex)
data$Embarked <- factor(data$Embarked)
data$SibSp <- factor(data$SibSp)
data$Parch <- factor(data$Parch)
data$cabin_status <- factor(data$cabin_status)
data$Parch_alt <- factor(data$Parch_alt)
data$SibSp_alt <- factor(data$SibSp_alt)
data$cabin_alt <- factor(data$cabin_alt)
data$title <- factor(data$title)

data$Age <- ifelse(is.na(data$Age) == TRUE,mean(data$Age,na.rm=TRUE),data$Age)
data$Fare <- ifelse(is.na(data$Fare) == TRUE,mean(data$Fare,na.rm=TRUE),data$Fare)

data <- data %>% select(Pclass, Embarked, SibSp_alt, Parch_alt,cabin_status,Fare,Age,title)

return(data)
}

temp <- prepare(train)
train <- data.frame(Survived = train$Survived,temp)
train$Survived <- factor(train$Survived)

```

## **Model Building**
Now that we have our key variables, let us train a few different models with the variables we identified above.  We will then compare the accuracy of these different models.  Namely, we will be comparing logistic regression, classification trees, and random forest methods.  

In the code below, you will the code I use for generating these models.  

* * *  

### **Classification Tree**    

* * *  

```{r look at tree method}
set.seed(1431)

tree <- tree(Survived~.,data=train)
plot(tree)
text(tree)

prediction_tree <- predict(tree, train, type="class")  # predicted scores
```  

* * *  

### **Logistic Regression**    

* * *  

```{r look at logistic method}
logistic <- glm(Survived~.,data=train,family='binomial')
summary(logistic)

prediction_logit <- predict(logistic, train, type="response")  # predicted scores

optCutOff <- optimalCutoff(train$Survived, prediction_logit)
prediction_logit <- ifelse(prediction_logit >= optCutOff,1,0)
prediction_logit <- factor(prediction_logit)
```

* * *  

### **Random Forest**    

* * *  



```{r look at random forest}
forest_model <- randomForest(Survived~., data=train, ntree=500, proximity=T)
prediction_forest <- predict(forest_model, train, type="response")  # predicted scores
```   

* * *  

### **Gradient  Boosting**    

* * *  


```{r look at boosting}
temp <- train
temp$Survived <- as.numeric(temp$Survived)

temp$Survived <- temp$Survived-1

boosting_model <- gbm(Survived~., data=temp, distribution='bernoulli',n.trees=10000,interaction.depth=8,shrinkage=0.001)
prediction_boost <- predict(boosting_model, train, n.trees=10000, type = 'response')  # predicted scores

optCutOff <- optimalCutoff(temp$Survived, prediction_boost, optimiseFor = 'misclasserror')
prediction_boost <- ifelse(prediction_boost >= optCutOff,1,0)
prediction_boost <- factor(prediction_boost)

remove(temp)
```  

Now that we have our models conditioned, lets compare the accuracy of our models.  We will compare our models on three key metrics:  

1. Misclassification Rates
2. Sensitivity
3. Specificity

```{r generate summary metrics for models, include = FALSE}
results <- data.frame(train,prediction_tree,prediction_logit,prediction_forest,prediction_boost)

results <- results %>% select(Survived,prediction_tree,prediction_logit,prediction_forest,prediction_boost)
positives <- results %>% filter(Survived == 1)
negatives <- results %>% filter(Survived == 0)


calculate_error <- function(results) {
results$tree <- ifelse(results$prediction_tree == results$Survived,'Correct','Incorrect')
results$logistic <- ifelse(results$prediction_logit == results$Survived,'Correct','Incorrect')
results$forest <- ifelse(results$prediction_forest == results$Survived,'Correct','Incorrect')
results$boost <- ifelse(results$prediction_boost == results$Survived,'Correct','Incorrect')

tree <- results %>% group_by(tree) %>% summarise(count = n()) %>% select(result = tree,count)
logit <- results %>% group_by(logistic) %>% summarise(count = n()) %>% select(result = logistic,count)
forest <- results %>% group_by(forest) %>% summarise(count = n()) %>% select(result = forest,count)
boost <- results %>% group_by(boost) %>% summarise(count = n()) %>% select(result = boost,count)

tree$method <- 'Classification Tree'
logit$method <- 'Logistic Regression'
forest$method <- 'Random Forest'
boost$method <- 'Gradient Boosting'


results <- rbind(tree,logit,forest,boost)
rm(tree,logit,forest,boost)

results <- results %>% group_by(method) %>% summarise(correct = max(count[result=='Correct']), wrong = max(count[result=='Incorrect'])) %>% mutate(error = wrong/(correct+wrong))
return(results)
}
overall <- calculate_error(results)
sensitivity <- calculate_error(positives)
specificity <- calculate_error(negatives)

misclass <- ggplot(overall,aes(x=method,y=error,fill=method)) + geom_bar(stat='identity') + xlab('') + ylab('Misclassification Rate') + scale_y_continuous(label=scales::percent) + theme(legend.position = 'none') + geom_text(aes(label=paste(round(error,digits=3)*100,'%')),position=position_dodge(0.9),vjust=-0.25)

sensitivity <- ggplot(sensitivity,aes(x=method,y=error,fill=method)) + geom_bar(stat='identity') + xlab('') + ylab('Sensitivity') + scale_y_continuous(label=scales::percent) + theme(legend.position = 'none') + geom_text(aes(label=paste(round(error,digits=3)*100,'%')),position=position_dodge(0.9),vjust=-0.25)

specificity <- ggplot(specificity,aes(x=method,y=error,fill=method)) + geom_bar(stat='identity') + xlab('') + ylab('Specificity') + scale_y_continuous(label=scales::percent) + theme(legend.position = 'none') + geom_text(aes(label=paste(round(error,digits=3)*100,'%')),position=position_dodge(0.9),vjust=-0.25)

```

```{r view key model summary metrics side by side, echo = FALSE}
grid.arrange(misclass, sensitivity, specificity, nrow=3)
```  

## **Predictions**  

Now that we have a clear winner in the gradient boosted model, lets take that model and apply it to the test data supplied by Kaggle.  

```{r prepare test data and make predictions}
test <- read.csv("/Users/hanson377/Documents/GitHub/kaggle_projects/Titanic/data/test.csv")
temp <- prepare(test)
test <- data.frame(PassengerId = test$PassengerId,temp)
rm(temp)

test$Survived <- predict(boosting_model, test, type="response")  # predicted scores
test$Survived <- ifelse(test$Survived >= optCutOff,1,0)

final <- test %>% select(PassengerId,Survived)
final$Survived <- as.numeric(final$Survived)
write.csv(final, "/Users/hanson377/Documents/GitHub/kaggle_projects/Titanic/data/gender_submission.csv", row.names=FALSE)
```
